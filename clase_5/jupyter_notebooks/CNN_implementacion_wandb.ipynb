{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wIQ8hjDpdVi"
      },
      "source": [
        "## Importar lo necesario y establecemos la configuracines necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "uHQUjDs12DLW"
      },
      "outputs": [],
      "source": [
        "import torch # pytorch\n",
        "import torchvision # para cargar datasets y transformaciones\n",
        "import random # para generar numeros aleatorios y probar nuetro modelo entrenado (buscando aleatoriamente ejemplos)\n",
        "import matplotlib.pyplot as plt # para graficar\n",
        "from tqdm import tqdm # para graficar la barra de avance\n",
        "import wandb # hacer log en whights and bias\n",
        "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts # modificar LR conforme se entrena"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biK7BQSsgEM0",
        "outputId": "ad75b842-42b3-4813-c3db-15ad4d28e03a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in c:\\uba\\uba_venv\\uba_venv\\lib\\site-packages (1.8.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# instalar torchinfo para ver el modelo y sus parametros\n",
        "!pip install torchinfo\n",
        "import torchinfo as torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeJy8fjPn4wi"
      },
      "source": [
        "### configuramos el `device` acorde al device disponible\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lOV9xybtn4I3",
        "outputId": "64b1e4d6-da1d-4eb2-995b-1c7f38342ae3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IROzIJYLho4P"
      },
      "source": [
        "### nos vinculamos al weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjEyZuHV97Iw",
        "outputId": "4af6bdd5-3114-42ec-af6e-9768b027d792"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\CIDIEE_NN\\.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlelectronfou\u001b[0m (\u001b[33mmmaillot\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# key de Marcos Uriel Maillot (lelectronfou@gmail.com), cámbiela a su usario una vez finalizada la clase.\n",
        "wandb.login(key=\"coloque-aqui-su-key-de-wandb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_tH9u082jpZ"
      },
      "source": [
        "\n",
        "\n",
        "## Ejemplo de red neuronal de convolución (CNN)\n",
        "**MNIST data base**\n",
        "Vamos a usar la base de datos de MNIST ([ver fuente](http://yann.lecun.com/exdb/mnist/)) para entrenar una CNN que identifique números escritos a mano.\n",
        "\n",
        "Para esto necesitamos:\n",
        "\n",
        "\n",
        "1.   Cargar la base de datos.\n",
        "2.   Ver que la base de datos esté ok.\n",
        "3.   Construir nuestra CNN.\n",
        "4. Ver que las dimensiones de la red sean consistentes.\n",
        "4.   Definir funciones necesarias (de entrenamiento, de costo, etc.).\n",
        "5. Entrenar la red.\n",
        "6. Ver que funcione.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nQ-MLk6Do8e"
      },
      "source": [
        "## 1. Cargar base de datos\n",
        "\n",
        "De la documentación, ver:\n",
        "\n",
        "\n",
        "Transformación `torchvision.transforms.ToTensor()`\n",
        "\n",
        "```\n",
        "... Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]...\n",
        "```\n",
        "\n",
        "Transformación `Normalize`\n",
        "\n",
        "```\n",
        "... Normalize a tensor image with mean and standard deviation. ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JvzatGF4e0W",
        "outputId": "e0ac94b7-1f7e-4db8-c679-943abe6176e7"
      },
      "outputs": [],
      "source": [
        "# definimos un conjunto de transformaciones para el dataset MNIST\n",
        "# (pasamos de imagenes de 28x28 a tensores, normalizamos, etc.)\n",
        "tranformaciones = torchvision.transforms.Compose([\n",
        "                            torchvision.transforms.ToTensor(),#<---------------- escala entre 0 y 1; pasa a tensor; poner canal en 1ra dim\n",
        "                            torchvision.transforms.Normalize((0.1307,), (0.3081,))#<----------------- normaliza el dataset; media y desviación estándar de los datos\n",
        "                            ])\n",
        "\n",
        "\n",
        "# creamos el dataset (para cargar los datos a procesar) MNIST de entrenamiento y test\n",
        "# (descargamos los datos si no están en la carpeta ../data)\n",
        "train_dataset = torchvision.datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=tranformaciones\n",
        "                      )\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST('../data', train=False,\n",
        "                   transform=tranformaciones\n",
        "                     )\n",
        "\n",
        "## aquí puede practicar por fuera de la clase, empleando otros datasets de torchvision.datasets, como CIFAR10, FashionMNIST, etc.\n",
        "# para eso debe cargar el dataset deseado, por ejemplo:\n",
        "# FASHIONMNIST, CIFAR10, etc. (ver https://pytorch.org/vision/stable/datasets.html)\n",
        "\n",
        "# si desea trabajar con FashionMNIST, puede usar el siguiente código (descomentar):\n",
        "# train_dataset = torchvision.datasets.FashionMNIST('../data', train=True, download=True, transform=tranformaciones)\n",
        "# test_dataset = torchvision.datasets.FashionMNIST('../data', train=False, download=True, transform=tranformaciones)\n",
        "\n",
        "# o si desea trabajar con CIFAR10, puede usar el siguiente código (descomentar):\n",
        "# train_dataset = torchvision.datasets.CIFAR10('../data', train=True, download=True, transform=tranformaciones)\n",
        "# test_dataset = torchvision.datasets.CIFAR10('../data', train=False, download=True, transform=tranformaciones)\n",
        "# \n",
        "# cuando corra estos otros datasets, recuerde que las transformaciones deben ser las adecuadas para cada dataset\n",
        "# y que su arquitectura de red debe ser acorde a las dimensiones de las imágenes y al número de clases del dataset.\n",
        "\n",
        "# ahora creamos el dataloader (recordar, dataloader -> una herramienta para hacer batches de datasets)\n",
        "dataloader = {\n",
        "    'train': torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True),\n",
        "    'test': torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oikthAE4Dteb"
      },
      "source": [
        "## 2. Ver que la base de datos esté OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyq2UFIl-Qjy",
        "outputId": "152c7595-f36c-4b31-dd0a-ca66a29f6b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ],
      "source": [
        "print(type(dataloader))\n",
        "print(type(dataloader['train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3dVPXQRch4xV"
      },
      "outputs": [],
      "source": [
        "# Ver imagen and label del dataloader \n",
        "train_features, train_labels = next(iter(dataloader['train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2fs6Qdivs1H",
        "outputId": "ef2ff95f-4a5b-424b-ede1-8d23bb9453ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del batch de feature (input / imagen): torch.Size([64, 1, 28, 28])\n",
            "Tamaño del batch del label (clase / etiqueta): torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# verifico sus dimensiones\n",
        "print(f\"Tamaño del batch de feature (input / imagen): {train_features.size()}\")\n",
        "print(f\"Tamaño del batch del label (clase / etiqueta): {train_labels.size()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "rfK_dXQdI2C6",
        "outputId": "c9f7f094-60b9-4a5b-a8cc-3abb337f33a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tamaño de 1 imagen:  torch.Size([1, 28, 28])\n",
            "tamaño de 1 imagen DESPUES de squeeze:  torch.Size([28, 28])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: 8\n"
          ]
        }
      ],
      "source": [
        "# tomo 1 imagen para poder visualizarla\n",
        "# y verifico sus dimensiones\n",
        "\n",
        "img = train_features[5]\n",
        "print('tamaño de 1 imagen: ', img.shape)\n",
        "# le QUITO 1 dimension (la del tamaño del batch) para poder graficar\n",
        "img = img.squeeze()\n",
        "print('tamaño de 1 imagen DESPUES de squeeze: ', img.shape)\n",
        "label = train_labels[5]\n",
        "\n",
        "# ploteo esa imagen\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFnOkkTgYTHV",
        "outputId": "972b1b92-8aab-479d-f99b-74fdac04d43a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel [0,0]:  tensor(-0.4242)\n",
            "pixel maximo:  tensor(2.8215)\n",
            "pixel minimo:  tensor(-0.4242)\n"
          ]
        }
      ],
      "source": [
        "print('pixel [0,0]: ',img[0][0])\n",
        "print('pixel maximo: ', torch.max(img))\n",
        "print('pixel minimo: ', torch.min(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLDYgFptiqPd"
      },
      "source": [
        "## 3. Construyo mi CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0TN4erDxRd"
      },
      "source": [
        "#### Bloque de convolución\n",
        "\n",
        "defino primero un \"bloque\" de una capa CNN\n",
        "construido con los bloques funcionales vistos en clase\n",
        "\n",
        "argumentos a pasar a la función:\n",
        "\n",
        "  - `c_in`:   canales (kernels) de entrada\n",
        "  - `c_out`:  canales (kernels) de salida\n",
        "  - `k`:      tamaño del kernel kxk\n",
        "  - `p`:      tamaño del padding de la convolución\n",
        "  - `s`:      stride de la convolución\n",
        "  - `pk`:     tamaño del kernel del pooling\n",
        "\n",
        "\n",
        "la función pooling se elige directamente dentro del bloque!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qMxa2DAsim9t"
      },
      "outputs": [],
      "source": [
        "# bloque de convolución para emplear en mi red\n",
        "# esta función la utilizaré en la clase del a CNN\n",
        "\n",
        "def conv_block(c_in, c_out, k=3, p='same', s=1, pk=2):        \n",
        "    return torch.nn.Sequential(                               # el módulo Sequential se engarga de hacer el forward de todo lo que tiene dentro.\n",
        "        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s), # conv\n",
        "        torch.nn.Tanh(),                                      # activation\n",
        "        torch.nn.MaxPool2d(pk)                                # pooling\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgPfrY8VivH8"
      },
      "source": [
        "### Red convolucional (modelo)\n",
        "\n",
        "\n",
        "Ahora SI construyo mi red... usando la clase CNN de pytorch\n",
        "revisar muy bien las dimensiones a emplear en cada capa y\n",
        "tener presente la reducción de las dimensiones.\n",
        "\n",
        "En la útlima capa fully conected `fc`, hacer bien el cálculo final del\n",
        "tamaño del array que se obtiene siguiendo la formula vista en la teoria\n",
        "tanto para la capa conv como para la capa pooling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrO5gfEL3KRC"
      },
      "outputs": [],
      "source": [
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self, n_channels=1, n_outputs=10):\n",
        "    super().__init__()\n",
        "    # aquí defino la arquitectura de mi red convolucional\n",
        "    self.conv1 = conv_block(c_in = n_channels, c_out = 16, k=9, p='same', s=1, pk=2)\n",
        "    self.conv1_out = None\n",
        "    self.drop = torch.nn.Dropout2d(p=0.7, inplace=False)\n",
        "    self.conv2 = conv_block(c_in = 16, c_out = 16, k=9, p='same', s=1, pk=2)\n",
        "    self.conv2_out = None\n",
        "    self.conv3 = conv_block(c_in = 16, c_out = 16, k=9, p='same', s=1, pk=2)\n",
        "    self.fc = torch.nn.Linear(16*3*3, n_outputs) # verificar la dim de la salida para calcular el tamaño de la fully conected!!\n",
        "\n",
        "    # aquí imprimo la arquitectura de la red y el número de parámetros entrenables\n",
        "    print('----------------------------------')\n",
        "    print('Red creada')\n",
        "    print('arquitectura:')\n",
        "    print(self)\n",
        "\n",
        "    # Me fijo en el número de capas\n",
        "    i=0\n",
        "    for layer in self.children():\n",
        "        i=i+1\n",
        "    print('----------------------------------')\n",
        "    print('Número total de capas de CNN (conv+act+polling) + finales : ', i)\n",
        "\n",
        "    # Me fijo en el número de parámetros entrenables\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print('----------------------------------')\n",
        "    print('Número total de parámetros a entrenar: ', pytorch_total_params)\n",
        "    print('----------------------------------')\n",
        "\n",
        "  # aquí defino el método forward, que es el que se ejecuta cuando llamo a la red con un input\n",
        "  def forward(self, x):\n",
        "    #print('input shape: ', x.shape)\n",
        "    self.conv1_out = self.drop(self.conv1(x))\n",
        "    self.conv2_out = self.drop(self.conv2(self.conv1_out))\n",
        "    self.conv3_out = self.conv3(self.conv2_out)\n",
        "    y = self.conv3_out\n",
        "    y = y.flatten(start_dim=1)\n",
        "    #print(y.shape)\n",
        "    y = self.fc(y)\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6DoGaP31md",
        "outputId": "92b13b78-5769-430d-e484-57e59c0f6b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "Red creada\n",
            "arquitectura:\n",
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(9, 9), stride=(1, 1), padding=same)\n",
            "    (1): Tanh()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (drop): Dropout2d(p=0.7, inplace=False)\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 16, kernel_size=(9, 9), stride=(1, 1), padding=same)\n",
            "    (1): Tanh()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(16, 16, kernel_size=(9, 9), stride=(1, 1), padding=same)\n",
            "    (1): Tanh()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=144, out_features=10, bias=True)\n",
            ")\n",
            "----------------------------------\n",
            "Número total de capas de CNN (conv+act+polling) + finales :  5\n",
            "----------------------------------\n",
            "Número total de parámetros a entrenar:  44266\n"
          ]
        }
      ],
      "source": [
        "# instancio modelo\n",
        "model = CNN()\n",
        "\n",
        "# armo config\n",
        "# OJO!! NO ESTÁ AUTOMATIZADO!\n",
        "# lo hago así para que tomen práctica sobre donde modificar en el código los \n",
        "# hyperparámetros de la red y el entrenamiento.\n",
        "model_config = {\n",
        "        \"num_layers\": 5,\n",
        "        \"kernel_size\": 9,\n",
        "        \"dropout\": 0.7,\n",
        "        \"n_channels\": [16, 16, 16],\n",
        "        \"architecture\": model.__class__.__name__\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYgRNa9M8cjI",
        "outputId": "421b44dd-c7f6-4d28-b6d5-6137942eeb1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_layers': 5,\n",
              " 'kernel_size': 9,\n",
              " 'dropout': 0.7,\n",
              " 'n_channels': [16, 16, 16],\n",
              " 'architecture': 'CNN'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4tEn-XqHVZ7"
      },
      "source": [
        "## 4. Veamos que las dimensiones sean consistentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q44d3Ftwokla",
        "outputId": "d3456984-d520-4aac-eafe-b3df5981b1a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CNN                                      [12, 10]                  --\n",
              "├─Sequential: 1-1                        [12, 16, 14, 14]          --\n",
              "│    └─Conv2d: 2-1                       [12, 16, 28, 28]          1,312\n",
              "│    └─Tanh: 2-2                         [12, 16, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-3                    [12, 16, 14, 14]          --\n",
              "├─Dropout2d: 1-2                         [12, 16, 14, 14]          --\n",
              "├─Sequential: 1-3                        [12, 16, 7, 7]            --\n",
              "│    └─Conv2d: 2-4                       [12, 16, 14, 14]          20,752\n",
              "│    └─Tanh: 2-5                         [12, 16, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-6                    [12, 16, 7, 7]            --\n",
              "├─Dropout2d: 1-4                         [12, 16, 7, 7]            --\n",
              "├─Sequential: 1-5                        [12, 16, 3, 3]            --\n",
              "│    └─Conv2d: 2-7                       [12, 16, 7, 7]            20,752\n",
              "│    └─Tanh: 2-8                         [12, 16, 7, 7]            --\n",
              "│    └─MaxPool2d: 2-9                    [12, 16, 3, 3]            --\n",
              "├─Linear: 1-6                            [12, 10]                  1,450\n",
              "==========================================================================================\n",
              "Total params: 44,266\n",
              "Trainable params: 44,266\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 73.37\n",
              "==========================================================================================\n",
              "Input size (MB): 0.04\n",
              "Forward/backward pass size (MB): 1.58\n",
              "Params size (MB): 0.18\n",
              "Estimated Total Size (MB): 1.80\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# corremos el resumen de la red para ver los parámetros y tamaños intermedios\n",
        "# es necesario pasarle el tamaño de la entrada, que en este caso es (batch_size, n_channels, height, width)\n",
        "torchinfo.summary(model, input_size=( 12, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoB3GvDtGUgY"
      },
      "source": [
        "## 5. Armo las funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# definimo optimizer y la función de pérdida\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# paso al config el optimizador\n",
        "model_config[\"optimizer\"] = optimizer.__class__.__name__\n",
        "# defino función de perdida\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Scheduler\n",
        "scheduler_type = \"step\"\n",
        "steps_per_epoch = len(dataloader['train'])\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=steps_per_epoch, T_mult=2, eta_min=1e-5)\n",
        "#scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-2, step_size_up=steps_per_epoch, mode=\"triangular2\")\n",
        "\n",
        "## Si queremos aplicar cambios al learning rate al final de cada epoch, usamos como ejemplo otro como este:\n",
        "#scheduler_type = \"epoch\"\n",
        "#scheduler = ExponentialLR(optimizer, gamma=0.99) # https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w-WYfZd-Yvc"
      },
      "outputs": [],
      "source": [
        "# armamos una función para entrenar el modelo con logeo de Weights & Biases (wandb)\n",
        "# a esta función hay que pasarle el modelo, el optimizador, los dataloaders de entrenamiento y evaluación, \n",
        "# la función de pérdida, el config y otros parámetros de configuración.\n",
        "\n",
        "def train_model_wandb(\n",
        "        model,\n",
        "        optimizer,\n",
        "        train_loader,\n",
        "        eval_loader,\n",
        "        loss_module,\n",
        "        config = model_config,\n",
        "        scheduler_type=\"step\",\n",
        "        patience=3,\n",
        "        patience_factor=0.01,\n",
        "        num_epochs=3,\n",
        "        scheduler = scheduler\n",
        "):\n",
        "    # Initialize Weights & Biases\n",
        "    # el nombre del proyecto es \"CEIA-Co[NUMERO_DE_COHORTE]\", y el config es el que armamos antes\n",
        "    # para que quede guardado en el log de wandb y podamos conocer los hyperparámetros empleados \n",
        "    # en ese entrenamiento\n",
        "    wandb.init(project=\"CEIA-Co19-CNN-mnist\", config=config)\n",
        "\n",
        "    ## Set device\n",
        "    model.to(device)\n",
        "\n",
        "    # Set metric for callbacks\n",
        "    best_eval = 0\n",
        "    pt_epoch = 0\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\"):\n",
        "        ### Training Phase ###\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_samples = 0\n",
        "\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "\n",
        "            X, y = data\n",
        "            # Move input data to device (if using GPU)\n",
        "            data_inputs = X.to(device)\n",
        "            data_labels = y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)  # Ensure shape consistency\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_module(preds, data_labels)\n",
        "\n",
        "            # Zero gradients, backpropagate, and update weights\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss for this batch\n",
        "            batch_loss = loss.item()\n",
        "            train_loss += batch_loss\n",
        "\n",
        "            # Compute accuracy (assuming classification task)\n",
        "            if preds.ndim == 2:  # Softmax case\n",
        "                preds_classes = preds.argmax(dim=1)\n",
        "            else:  # Sigmoid case (binary classification)\n",
        "                preds_classes = (preds > 0.5).long()\n",
        "\n",
        "            train_correct += (preds_classes == data_labels).sum().item()\n",
        "            train_samples += data_labels.size(0)\n",
        "\n",
        "            # Log batch-wise metrics\n",
        "            wandb.log({\n",
        "                \"batch_loss\": batch_loss,\n",
        "                \"batch_step\": epoch * len(train_loader) + batch_idx,\n",
        "                \"batch_lr\": optimizer.param_groups[0]['lr'],\n",
        "            })\n",
        "\n",
        "            # Step-level callback\n",
        "            if scheduler_type==\"step\":\n",
        "                scheduler.step()\n",
        "\n",
        "        # Compute training metrics\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy = train_correct / train_samples\n",
        "\n",
        "        ### Evaluation Phase ###\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "        eval_correct = 0\n",
        "        eval_samples = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient tracking\n",
        "            for data in eval_loader:\n",
        "\n",
        "                X, y = data\n",
        "                data_inputs = X.to(device)\n",
        "                data_labels = y.to(device)\n",
        "\n",
        "                preds = model(data_inputs)\n",
        "                preds = preds.squeeze(dim=1)\n",
        "\n",
        "                loss = loss_module(preds, data_labels)\n",
        "                eval_loss += loss.item()\n",
        "\n",
        "                if preds.ndim == 2:\n",
        "                    preds_classes = preds.argmax(dim=1)\n",
        "                else:\n",
        "                    preds_classes = (preds > 0.5).long()\n",
        "\n",
        "                eval_correct += (preds_classes == data_labels).sum().item()\n",
        "                eval_samples += data_labels.size(0)\n",
        "\n",
        "        # Compute evaluation metrics\n",
        "        eval_loss /= len(eval_loader)\n",
        "        eval_accuracy = eval_correct / eval_samples\n",
        "\n",
        "        # Log epoch-level metrics\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"eval_loss\": eval_loss,\n",
        "            \"eval_accuracy\": eval_accuracy,\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Train Acc: {train_accuracy:.4f} | Eval Loss: {eval_loss:.4f} - Eval Acc: {eval_accuracy:.4f}\")\n",
        "\n",
        "        # Callbacks\n",
        "        ## Learning rate scheduler\n",
        "        if scheduler_type==\"epoch\":\n",
        "            scheduler.step()\n",
        "        ## Early stopping\n",
        "        if eval_accuracy>=best_eval*(1+patience_factor):\n",
        "            best_eval = eval_accuracy\n",
        "            pt_epoch = 0\n",
        "        else:\n",
        "            pt_epoch += 1\n",
        "            if pt_epoch>=patience:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} - Training interrupted due to early stopping condition.\")\n",
        "                wandb.finish()\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} - Current epochs without validation metric improvement {pt_epoch}. {patience-pt_epoch} remaining before stopping.\")\n",
        "\n",
        "    # Finish W&B run\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cW5H0nX99DW"
      },
      "source": [
        "## 6. Entrenamiento WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "47ad8Ydj-ePT",
        "outputId": "3654edeb-e1db-45db-8bef-eaa1b5a450e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\UBA\\aprendizaje_profundo\\clase_5\\jupyter_notebooks\\wandb\\run-20250526_213659-4nhwqyv3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mmaillot/CEIA-Co19/runs/4nhwqyv3' target=\"_blank\">fallen-terrain-1</a></strong> to <a href='https://wandb.ai/mmaillot/CEIA-Co19' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mmaillot/CEIA-Co19' target=\"_blank\">https://wandb.ai/mmaillot/CEIA-Co19</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mmaillot/CEIA-Co19/runs/4nhwqyv3' target=\"_blank\">https://wandb.ai/mmaillot/CEIA-Co19/runs/4nhwqyv3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:   7%|▋         | 1/15 [00:18<04:21, 18.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Train Loss: 1.5317 - Train Acc: 0.5672 | Eval Loss: 0.8548 - Eval Acc: 0.7771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  13%|█▎        | 2/15 [00:36<03:55, 18.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/15 - Train Loss: 0.7434 - Train Acc: 0.8007 | Eval Loss: 0.4166 - Eval Acc: 0.8816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  20%|██        | 3/15 [00:54<03:37, 18.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/15 - Train Loss: 0.5403 - Train Acc: 0.8534 | Eval Loss: 0.3618 - Eval Acc: 0.8968\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  27%|██▋       | 4/15 [01:12<03:18, 18.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/15 - Train Loss: 0.4449 - Train Acc: 0.8764 | Eval Loss: 0.2553 - Eval Acc: 0.9243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  33%|███▎      | 5/15 [01:30<03:01, 18.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/15 - Train Loss: 0.3644 - Train Acc: 0.8975 | Eval Loss: 0.2149 - Eval Acc: 0.9342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  40%|████      | 6/15 [01:49<02:44, 18.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/15 - Train Loss: 0.3296 - Train Acc: 0.9062 | Eval Loss: 0.1996 - Eval Acc: 0.9397\n",
            "Epoch 6/15 - Current epochs without validation metric improvement 1. 2 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  47%|████▋     | 7/15 [02:07<02:24, 18.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/15 - Train Loss: 0.3142 - Train Acc: 0.9099 | Eval Loss: 0.1944 - Eval Acc: 0.9408\n",
            "Epoch 7/15 - Current epochs without validation metric improvement 2. 1 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  53%|█████▎    | 8/15 [02:24<02:05, 17.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/15 - Train Loss: 0.2963 - Train Acc: 0.9141 | Eval Loss: 0.1700 - Eval Acc: 0.9469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  60%|██████    | 9/15 [02:42<01:46, 17.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/15 - Train Loss: 0.2699 - Train Acc: 0.9195 | Eval Loss: 0.1538 - Eval Acc: 0.9513\n",
            "Epoch 9/15 - Current epochs without validation metric improvement 1. 2 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  67%|██████▋   | 10/15 [02:59<01:28, 17.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/15 - Train Loss: 0.2494 - Train Acc: 0.9262 | Eval Loss: 0.1411 - Eval Acc: 0.9545\n",
            "Epoch 10/15 - Current epochs without validation metric improvement 2. 1 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  73%|███████▎  | 11/15 [03:17<01:11, 17.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/15 - Train Loss: 0.2361 - Train Acc: 0.9291 | Eval Loss: 0.1334 - Eval Acc: 0.9568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  80%|████████  | 12/15 [03:36<00:53, 17.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/15 - Train Loss: 0.2259 - Train Acc: 0.9328 | Eval Loss: 0.1295 - Eval Acc: 0.9574\n",
            "Epoch 12/15 - Current epochs without validation metric improvement 1. 2 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  87%|████████▋ | 13/15 [03:54<00:36, 18.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/15 - Train Loss: 0.2191 - Train Acc: 0.9354 | Eval Loss: 0.1262 - Eval Acc: 0.9590\n",
            "Epoch 13/15 - Current epochs without validation metric improvement 2. 1 remaining before stopping.\n",
            "Epoch 14/15 - Train Loss: 0.2147 - Train Acc: 0.9354 | Eval Loss: 0.1251 - Eval Acc: 0.9599\n",
            "Epoch 14/15 - Training interrupted due to early stopping condition.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▂▁▁▂▁▂▁▂▁</td></tr><tr><td>batch_lr</td><td>▆▁▁█▇▅▄▂▁▁█████▆▆▅▄▂▁▁▁████▇▇▇▆▆▆▅▅▃▃▂▂▁</td></tr><tr><td>batch_step</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>eval_accuracy</td><td>▁▅▆▇▇▇▇███████</td></tr><tr><td>eval_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>█▄█▇▄▂██▇▆▄▃▂▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.22448</td></tr><tr><td>batch_lr</td><td>1e-05</td></tr><tr><td>batch_step</td><td>13131</td></tr><tr><td>epoch</td><td>13</td></tr><tr><td>eval_accuracy</td><td>0.9599</td></tr><tr><td>eval_loss</td><td>0.12508</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>train_accuracy</td><td>0.93538</td></tr><tr><td>train_loss</td><td>0.21468</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fallen-terrain-1</strong> at: <a href='https://wandb.ai/mmaillot/CEIA-Co19/runs/4nhwqyv3' target=\"_blank\">https://wandb.ai/mmaillot/CEIA-Co19/runs/4nhwqyv3</a><br> View project at: <a href='https://wandb.ai/mmaillot/CEIA-Co19' target=\"_blank\">https://wandb.ai/mmaillot/CEIA-Co19</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250526_213659-4nhwqyv3\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  87%|████████▋ | 13/15 [04:15<00:39, 19.63s/it]\n"
          ]
        }
      ],
      "source": [
        "# corremos la función de entrenamiento con todos los argumentos correspondientes\n",
        "train_model_wandb(model, optimizer, dataloader['train'], dataloader['test'], loss_fn, scheduler_type=scheduler_type, num_epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHcr6aDtHNfc"
      },
      "source": [
        "## 7. Vemos que funcione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "k_jFvJ603PC-",
        "outputId": "2961c411-8eeb-4eb8-f902-cc083df20d3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del batch de feature (input / imagen): 64\n",
            "torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño imagen de entrada a red:  torch.Size([1, 1, 28, 28])\n",
            "Predición del modelo:\n",
            "tensor([[-0.1152, -1.5734,  7.9792,  2.0501, -4.7255,  0.6155, -3.4550, -0.6987,\n",
            "         -0.7724, -1.9042]], device='cuda:0')\n",
            "\n",
            "softmax de predicción:\n",
            "tensor([[3.0401e-04, 7.0729e-05, 9.9595e-01, 2.6499e-03, 3.0245e-06, 6.3129e-04,\n",
            "         1.0775e-05, 1.6961e-04, 1.5756e-04, 5.0807e-05]], device='cuda:0')\n",
            "\n",
            "El numero es un:  2\n"
          ]
        }
      ],
      "source": [
        "# corremos 1 dato, a ver como lo clasifica...\n",
        "# generamos un batch del dataloader de testeo\n",
        "test_features, test_labels = next(iter(dataloader['test']))\n",
        "# obtengo el tamaño del batch\n",
        "print(f\"Tamaño del batch de feature (input / imagen): {test_features.size(0)}\")\n",
        "\n",
        "# generamos un índice aleatorio para tomar una imagen del batch\n",
        "k = random.randint(0, test_features.size(0) - 1)\n",
        "\n",
        "# verifico las dimensiones \n",
        "samp_img = test_features[k]\n",
        "print(samp_img.shape)\n",
        "\n",
        "\n",
        "# ploteo la imagen\n",
        "plt.imshow(samp_img.squeeze(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# preparo para pasarla a la red (model) asi predice.\n",
        "samp_imp = samp_img.unsqueeze(0) # agrego la batch dim \n",
        "samp_img = samp_imp.to(device) # aseguro que la imagen esté en el device correcto (GPU o CPU)\n",
        "print('Tamaño imagen de entrada a red: ', samp_img.shape)\n",
        "\n",
        "# la paso al modelo\n",
        "model.to(device)\n",
        "model.eval()\n",
        "y_hat = model(samp_img)\n",
        "print('Predición del modelo:')\n",
        "print(y_hat.detach())\n",
        "print()\n",
        "print('softmax de predicción:')\n",
        "print(torch.nn.functional.softmax(y_hat, dim=1).detach())\n",
        "print()\n",
        "print(f'El numero es un: ', torch.argmax(y_hat, axis=1).item())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "UBAvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
