{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blue Book for Bulldozers dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZARAUZ\\AppData\\Local\\Temp\\ipykernel_12956\\3728147424.py:22: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_data)\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Name of the CSV file inside the ZIP archive\n",
    "csv_file_name = 'Train.csv'\n",
    "\n",
    "# Open the ZIP file in read mode\n",
    "with zipfile.ZipFile('Train.csv.zip', 'r') as zip_file:\n",
    "    # Check if the CSV file exists in the archive\n",
    "    if 'Train.csv' not in zip_file.namelist():\n",
    "        raise FileNotFoundError(f\"The file `Train.csv` does not exist in the ZIP archive.\")\n",
    "\n",
    "    # Open the CSV file inside the ZIP archive\n",
    "    with zip_file.open('Train.csv') as csv_file:\n",
    "        # Wrap the binary data in a TextIOWrapper to decode it as text\n",
    "        csv_data = io.TextIOWrapper(csv_file, encoding='utf-8')\n",
    "        \n",
    "        # Use pandas to read the CSV content\n",
    "        df = pd.read_csv(csv_data)\n",
    "\n",
    "# Preprocessing Step 1: Remove columns with more than 50% null values\n",
    "threshold = 0.5 * len(df)  # 50% of the total rows\n",
    "df = df.loc[:, df.isnull().sum() <= threshold]\n",
    "\n",
    "# Preprocessing Step 2: Remove columns ending with 'Desc' or 'ID'\n",
    "columns_to_drop = [col for col in df.columns if col.endswith('Desc') or col.endswith('ID')]\n",
    "df = df.drop(columns=columns_to_drop + ['datasource'])\n",
    "\n",
    "# Preprocessing Step 3: Convert `saledate` to datetime and handle cyclic transformations\n",
    "if 'saledate' in df.columns:\n",
    "    # Convert `saledate` to datetime format\n",
    "    df['saledate'] = pd.to_datetime(df['saledate'], format='%m/%d/%Y %H:%M')\n",
    "\n",
    "    # Extract cyclic components (hour, day, month)\n",
    "    timestamp = df['saledate']\n",
    "    days = timestamp.dt.day\n",
    "    months = timestamp.dt.month\n",
    "    years = timestamp.dt.year\n",
    "\n",
    "    # Define maximum values for each cyclic component\n",
    "    max_day = 31\n",
    "    max_month = 12\n",
    "\n",
    "    # Apply sine and cosine transformations for each cyclic component\n",
    "    df['sale_day_sin'] = np.sin(2 * np.pi * days / max_day)\n",
    "    df['sale_day_cos'] = np.cos(2 * np.pi * days / max_day)\n",
    "\n",
    "    df['sale_month_sin'] = np.sin(2 * np.pi * months / max_month)\n",
    "    df['sale_month_cos'] = np.cos(2 * np.pi * months / max_month)\n",
    "\n",
    "    df['sale_year_sin'] = np.sin(2 * np.pi * (years - years.min()) / (years.max() - years.min()))\n",
    "    df['sale_year_cos'] = np.cos(2 * np.pi * (years - years.min()) / (years.max() - years.min()))\n",
    "\n",
    "    # Drop the original `saledate` column\n",
    "    df = df.drop(columns=['saledate', 'Coupler', 'fiBaseModel'])\n",
    "\n",
    "# Preprocessing Step 4: Decompose `state` into latitude and longitude\n",
    "with open('state_coordinates.json', 'r', encoding='utf-8') as f:\n",
    "    state_coordinates = json.load(f)\n",
    "if 'state' in df.columns:\n",
    "    # Map state names to their latitude and longitude\n",
    "    df['latitude'] = df['state'].map(lambda x: state_coordinates.get(x, (None, None))[0])\n",
    "    df['longitude'] = df['state'].map(lambda x: state_coordinates.get(x, (None, None))[1])\n",
    "\n",
    "    # Drop the original `state` column\n",
    "    df = df.dropna().drop(columns=['state'])\n",
    "\n",
    "# Preprocessing Step 5: `YearMade` variable with a hybrid approach\n",
    "df['yearmade_norm'] = (df['YearMade'] - df['YearMade'].min()) / (df['YearMade'].max() - df['YearMade'].min())\n",
    "df['yearmade_sin'] = np.sin(2 * np.pi * df['yearmade_norm'])\n",
    "df['yearmade_cos'] = np.cos(2 * np.pi * df['yearmade_norm'])\n",
    "df = df.dropna().drop(columns=['YearMade'])\n",
    "\n",
    "\n",
    "# Preprocessing Step 6: `Hydraulics`, `ProductGroup` and `Enclosure` mapping for embedding layers\n",
    "## Build mappings\n",
    "hydraulics2id = {k:i for i, k in enumerate(df['Hydraulics'].unique())}\n",
    "enclosure2id = {k:i for i, k in enumerate(df['Enclosure'].unique())}\n",
    "productgroup2id = {k:i for i, k in enumerate(df['ProductGroup'].unique())}\n",
    "## Replace columns values\n",
    "df['Hydraulics'] = df['Hydraulics'].apply(lambda x: hydraulics2id[x])\n",
    "df['Enclosure'] = df['Enclosure'].apply(lambda x: enclosure2id[x])\n",
    "df['ProductGroup'] = df['ProductGroup'].apply(lambda x: productgroup2id[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For handling year-based features like `YearMade` and the `year` component from `saledate`, there are three main approaches, each with distinct advantages and trade-offs. Let's analyze them in depth:\n",
    "\n",
    "* Linear Normalization (Pure Numerical): Treat the year as a continuous numerical feature, normalized to `[0, 1]` or standardized.\n",
    "\n",
    "    ```python\n",
    "    df['yearmade_norm'] = (df['YearMade'] - df['YearMade'].min()) / (df['YearMade'].max() - df['YearMade'].min())\n",
    "    ```\n",
    "    \n",
    "    Pros:\n",
    "    - Simple and interpretable: Preserves the linear relationship (\"newer = more expensive\").\n",
    "    - Works well for monotonic trends: If bulldozer prices consistently increase/decrease with year, this captures it directly.\n",
    "    - No information loss: Retains the exact ordinal relationship between years.\n",
    "\n",
    "    Cons:\n",
    "    - Misses cyclical patterns: If there are periodic trends (e.g., economic cycles every 10 years), this won't capture them.\n",
    "    - Assumes linearity: May fail if the relationship isn’t strictly linear (e.g., prices plateau for newer models).\n",
    "\n",
    "    Best for:\n",
    "    - Strong, consistent linear trends (e.g., equipment depreciation).\n",
    "    - When other features already capture cyclicality (e.g., you’ve included macroeconomic features).\n",
    "\n",
    "* Cyclic Encoding (Sine-Cosine Transformation): Decompose the year into two features using sine/cosine to preserve cyclicality.\n",
    "\n",
    "    ```python\n",
    "    min_year = df['YearMade'].min()\n",
    "    max_year_span = df['YearMade'].max() - min_year\n",
    "    df['yearmade_sin'] = np.sin(2 * np.pi * (df['YearMade'] - min_year) / max_year_span)\n",
    "    df['yearmade_cos'] = np.cos(2 * np.pi * (df['YearMade'] - min_year) / max_year_span)\n",
    "    ```\n",
    "    \n",
    "    Pros:\n",
    "    - Captures cyclicality: Useful if year interacts with external trends (e.g., economic booms/busts).\n",
    "    - No ordinal assumption: Lets the model decide if \"older vs. newer\" matters more than absolute year.\n",
    "    - Smooth transitions: Neighboring years are similar in encoding space (e.g., 2023 $\\approx$ 2022).\n",
    "\n",
    "    Cons:\n",
    "    - Loses linearity: Harder for the model to learn \"newer = more expensive\" directly.\n",
    "    - Less interpretable: Hard to explain how the sine/cosine features relate to price.\n",
    "    - Requires more features: Doubles the dimensionality vs. linear encoding.\n",
    "\n",
    "    Best for:\n",
    "    - Long-term cyclical trends (e.g., vintage equipment appreciation).\n",
    "    - Datasets where year interacts with external factors (e.g., policy changes, technology shifts).\n",
    "\n",
    "* Hybrid Approach (Linear + Cyclic): Combine both linear and cyclic encodings to capture linear trends and cyclicality.\n",
    "\n",
    "    ```python\n",
    "    # Linear\n",
    "    df['yearmade_norm'] = (df['YearMade'] - df['YearMade'].min()) / (df['YearMade'].max() - df['YearMade'].min())\n",
    "    # Cyclic\n",
    "    df['yearmade_sin'] = np.sin(2 * np.pi * df['YearMade_norm'])\n",
    "    df['yearmade_cos'] = np.cos(2 * np.pi * df['YearMade_norm'])\n",
    "    ```\n",
    "    \n",
    "    Pros:\n",
    "    - Best of both worlds: Retains linearity while capturing cyclical patterns.\n",
    "    - Flexible: Lets the model weigh which relationship matters more.\n",
    "    - Robust: Works well even if one assumption (linearity/cyclicality) is wrong.\n",
    "\n",
    "    Cons:\n",
    "    - Higher dimensionality: Triples the feature count (original + sin + cos).\n",
    "    - Potential redundancy: If the relationship is purely linear, cyclic features add noise.\n",
    "\n",
    "    Best for:\n",
    "    - Cases where both linear and cyclical effects might exist (e.g., bulldozer prices generally increase with year but dip during recessions).\n",
    "    - When you're unsure which encoding is optimal.\n",
    "\n",
    "\n",
    "In our case, we will use a hybrid approach for `YearMade` and a cyclic one for `year` of `saledate`.\n",
    "\n",
    "Let's dive deeper into label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x21a8766d0d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.displot(df['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target variable has a strong correlation with Tweedie distributions, therefore we apply a simple logarithmic transformation to _normalize_ it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m mu, sigma = df[\u001b[33m'\u001b[39m\u001b[33mSalePrice_log1p\u001b[39m\u001b[33m'\u001b[39m].mean(), df[\u001b[33m'\u001b[39m\u001b[33mSalePrice_log1p\u001b[39m\u001b[33m'\u001b[39m].std()\n\u001b[32m      7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mSalePrice_log1p_norm\u001b[39m\u001b[33m'\u001b[39m] = (df[\u001b[33m'\u001b[39m\u001b[33mSalePrice_log1p\u001b[39m\u001b[33m'\u001b[39m]-mu)/sigma\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43msns\u001b[49m.displot(df[\u001b[33m'\u001b[39m\u001b[33mSalePrice_log1p_norm\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "target_var_transform = lambda x: np.log1p(x)\n",
    "\n",
    "df['SalePrice_log1p'] = df['SalePrice'].apply(lambda x: target_var_transform(x))\n",
    "\n",
    "mu, sigma = df['SalePrice_log1p'].mean(), df['SalePrice_log1p'].std()\n",
    "\n",
    "df['SalePrice_log1p_norm'] = (df['SalePrice_log1p']-mu)/sigma\n",
    "\n",
    "sns.displot(df['SalePrice_log1p_norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>ProductGroup</th>\n",
       "      <th>Enclosure</th>\n",
       "      <th>Hydraulics</th>\n",
       "      <th>sale_day_sin</th>\n",
       "      <th>sale_day_cos</th>\n",
       "      <th>sale_month_sin</th>\n",
       "      <th>sale_month_cos</th>\n",
       "      <th>sale_year_sin</th>\n",
       "      <th>sale_year_cos</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>yearmade_norm</th>\n",
       "      <th>yearmade_sin</th>\n",
       "      <th>yearmade_cos</th>\n",
       "      <th>SalePrice_log1p</th>\n",
       "      <th>SalePrice_log1p_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.101168</td>\n",
       "      <td>-0.994869</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-9.898214e-01</td>\n",
       "      <td>0.142315</td>\n",
       "      <td>32.806671</td>\n",
       "      <td>-86.791130</td>\n",
       "      <td>0.991115</td>\n",
       "      <td>-0.055794</td>\n",
       "      <td>0.998442</td>\n",
       "      <td>11.097425</td>\n",
       "      <td>1.259911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.848644</td>\n",
       "      <td>0.528964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-9.096320e-01</td>\n",
       "      <td>-0.415415</td>\n",
       "      <td>35.630066</td>\n",
       "      <td>-79.806419</td>\n",
       "      <td>0.983218</td>\n",
       "      <td>-0.105248</td>\n",
       "      <td>0.994446</td>\n",
       "      <td>10.950824</td>\n",
       "      <td>1.062206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.848644</td>\n",
       "      <td>0.528964</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-9.096320e-01</td>\n",
       "      <td>-0.415415</td>\n",
       "      <td>42.165726</td>\n",
       "      <td>-74.948051</td>\n",
       "      <td>0.988154</td>\n",
       "      <td>-0.074362</td>\n",
       "      <td>0.997231</td>\n",
       "      <td>9.210440</td>\n",
       "      <td>-1.284855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.651372</td>\n",
       "      <td>-0.758758</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>-1.133108e-15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.054487</td>\n",
       "      <td>-97.563461</td>\n",
       "      <td>0.988154</td>\n",
       "      <td>-0.074362</td>\n",
       "      <td>0.997231</td>\n",
       "      <td>10.558439</td>\n",
       "      <td>0.533041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.998717</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>-5.406408e-01</td>\n",
       "      <td>0.841254</td>\n",
       "      <td>42.165726</td>\n",
       "      <td>-74.948051</td>\n",
       "      <td>0.994077</td>\n",
       "      <td>-0.037207</td>\n",
       "      <td>0.999308</td>\n",
       "      <td>9.305741</td>\n",
       "      <td>-1.156333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalePrice  ProductGroup  Enclosure  Hydraulics  sale_day_sin  sale_day_cos  \\\n",
       "0      66000             0          0           0     -0.101168     -0.994869   \n",
       "1      57000             0          0           0     -0.848644      0.528964   \n",
       "2      10000             1          1           1     -0.848644      0.528964   \n",
       "3      38500             2          0           0     -0.651372     -0.758758   \n",
       "4      11000             1          2           1     -0.998717     -0.050649   \n",
       "\n",
       "   sale_month_sin  sale_month_cos  sale_year_sin  sale_year_cos   latitude  \\\n",
       "0       -0.500000    8.660254e-01  -9.898214e-01       0.142315  32.806671   \n",
       "1        1.000000    6.123234e-17  -9.096320e-01      -0.415415  35.630066   \n",
       "2        0.866025    5.000000e-01  -9.096320e-01      -0.415415  42.165726   \n",
       "3        0.500000   -8.660254e-01  -1.133108e-15       1.000000  31.054487   \n",
       "4       -0.500000   -8.660254e-01  -5.406408e-01       0.841254  42.165726   \n",
       "\n",
       "   longitude  yearmade_norm  yearmade_sin  yearmade_cos  SalePrice_log1p  \\\n",
       "0 -86.791130       0.991115     -0.055794      0.998442        11.097425   \n",
       "1 -79.806419       0.983218     -0.105248      0.994446        10.950824   \n",
       "2 -74.948051       0.988154     -0.074362      0.997231         9.210440   \n",
       "3 -97.563461       0.988154     -0.074362      0.997231        10.558439   \n",
       "4 -74.948051       0.994077     -0.037207      0.999308         9.305741   \n",
       "\n",
       "   SalePrice_log1p_norm  \n",
       "0              1.259911  \n",
       "1              1.062206  \n",
       "2             -1.284855  \n",
       "3              0.533041  \n",
       "4             -1.156333  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Main class\n",
    "class BBBDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert all numerical features to float32 in one operation for efficiency\n",
    "        numerical_features = {\n",
    "            k: torch.tensor(self.X.loc[idx, k], dtype=torch.float32)\n",
    "            for k in [\n",
    "                'sale_day_sin', 'sale_day_cos',\n",
    "                'sale_month_sin', 'sale_month_cos',\n",
    "                'sale_year_sin', 'sale_year_cos',\n",
    "                'latitude', 'longitude',\n",
    "                'yearmade_norm', 'yearmade_sin', 'yearmade_cos'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Categorical features remain as long\n",
    "        categorical_features = {\n",
    "            k: torch.tensor(self.X.loc[idx, k], dtype=torch.long)\n",
    "            for k in ['ProductGroup', 'Enclosure', 'Hydraulics']\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            **categorical_features,\n",
    "            **numerical_features,\n",
    "            'label': torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Data collator\n",
    "class BBBCollator:\n",
    "    def __init__(self):\n",
    "        self.float_features = [\n",
    "            'sale_day_sin', 'sale_day_cos',\n",
    "            'sale_month_sin', 'sale_month_cos',\n",
    "            'sale_year_sin', 'sale_year_cos',\n",
    "            'latitude', 'longitude',\n",
    "            'yearmade_norm', 'yearmade_sin', 'yearmade_cos'\n",
    "        ]\n",
    "        self.cat_features = ['ProductGroup', 'Enclosure', 'Hydraulics']\n",
    "    \n",
    "    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:\n",
    "        # Stack all float features into single tensor\n",
    "        X_float = torch.stack([\n",
    "            torch.stack([item[feat] for feat in self.float_features])\n",
    "            for item in batch\n",
    "        ]).float()\n",
    "        \n",
    "        # Prepare categorical features as separate tensors\n",
    "        X_cat = {\n",
    "            feat: torch.stack([item[feat] for item in batch]).long()\n",
    "            for feat in self.cat_features\n",
    "        }\n",
    "        \n",
    "        # Prepare target\n",
    "        y = torch.stack([item['label'] for item in batch]).float()\n",
    "        \n",
    "        return {'X_float': X_float, 'X_cat': X_cat}, y\n",
    "\n",
    "# Check implementation\n",
    "X, y = df[\n",
    "    [\n",
    "        'ProductGroup',\n",
    "        'Enclosure',\n",
    "        'Hydraulics',\n",
    "        'sale_day_sin',\n",
    "        'sale_day_cos',\n",
    "        'sale_month_sin',\n",
    "        'sale_month_cos',\n",
    "        'sale_year_sin',\n",
    "        'sale_year_cos',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'yearmade_norm',\n",
    "        'yearmade_sin',\n",
    "        'yearmade_cos'\n",
    "    ]\n",
    "], df['SalePrice_log1p_norm'].values\n",
    "\n",
    "train_dts = BBBDataset(X[:int(len(X)*0.8)].reset_index(drop=True), y[:int(len(X)*0.8)])\n",
    "val_dts = BBBDataset(X[int(len(X)*0.8):].reset_index(drop=True), y[int(len(X)*0.8):])\n",
    "collator = BBBCollator()\n",
    "train_dtl = DataLoader(train_dts, batch_size=128, shuffle=True, collate_fn=collator)\n",
    "val_dtl = DataLoader(val_dts, batch_size=256, shuffle=False, collate_fn=collator)\n",
    "\n",
    "batch_X, batch_y = next(iter(train_dtl))\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch_X, batch_y = {\"X_float\": batch_X[\"X_float\"].to(device), \"X_cat\": {k:v.to(device) for k,v in batch_X[\"X_cat\"].items()}}, batch_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_float': tensor([[-0.6514, -0.7588,  0.5000,  ...,  0.9625, -0.2335,  0.9724],\n",
       "         [ 0.1012, -0.9949, -1.0000,  ...,  0.9566, -0.2695,  0.9630],\n",
       "         [ 0.9885,  0.1514,  0.8660,  ...,  0.9576, -0.2636,  0.9646],\n",
       "         ...,\n",
       "         [-0.9987, -0.0506, -1.0000,  ...,  0.9931, -0.0434,  0.9991],\n",
       "         [-0.5713,  0.8208,  0.5000,  ...,  0.9911, -0.0558,  0.9984],\n",
       "         [ 0.3944,  0.9190,  1.0000,  ...,  0.9862, -0.0867,  0.9962]],\n",
       "        device='cuda:0'),\n",
       " 'X_cat': {'ProductGroup': tensor([2, 0, 4, 4, 4, 0, 1, 2, 3, 0, 1, 3, 3, 0, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3,\n",
       "          3, 0, 2, 3, 3, 3, 2, 2, 3, 0, 2, 1, 0, 0, 3, 2, 0, 3, 0, 2, 2, 3, 2, 3,\n",
       "          2, 1, 3, 0, 0, 1, 4, 0, 1, 1, 1, 2, 3, 2, 2, 2, 0, 0, 0, 4, 0, 1, 1, 0,\n",
       "          3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 4, 3, 2, 0, 4, 0, 3, 2, 2, 0, 4, 4, 3,\n",
       "          1, 2, 3, 2, 0, 4, 3, 2, 3, 3, 3, 0, 0, 2, 4, 4, 3, 3, 1, 2, 1, 1, 3, 3,\n",
       "          4, 2, 4, 2, 0, 2, 1, 4], device='cuda:0'),\n",
       "  'Enclosure': tensor([2, 1, 2, 0, 0, 1, 1, 2, 1, 0, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 0, 2, 1,\n",
       "          1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 1,\n",
       "          0, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 2, 1, 2, 0, 2, 1, 2, 0, 0, 0, 1, 2, 1,\n",
       "          1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 0, 1, 0, 2, 0, 2, 1, 2, 0, 2, 2, 1, 1,\n",
       "          1, 2, 1, 2, 0, 2, 2, 2, 1, 1, 2, 2, 0, 2, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "          1, 2, 2, 2, 0, 0, 1, 0], device='cuda:0'),\n",
       "  'Hydraulics': tensor([ 2,  0,  3,  3, 10,  0,  2,  1,  0,  0,  2,  0,  5,  0,  2,  2,  0,  2,\n",
       "           2,  0,  2,  2,  2,  0,  0,  0,  2,  0,  6,  6,  2,  2,  0,  0,  2,  2,\n",
       "           0,  0,  0,  2,  0,  0,  0,  2,  2,  0,  2,  0,  2,  2,  0,  0,  6,  1,\n",
       "           3,  0,  2,  2,  2,  2,  0,  2,  0,  1,  0,  0,  0,  3,  5,  2,  1,  0,\n",
       "           0,  2,  3,  0,  2,  0,  0,  0,  2,  2,  2,  3,  0,  2,  0,  3,  0,  0,\n",
       "           1,  0,  6,  3,  3,  0,  2,  1,  0,  2,  0,  3,  0,  2,  0,  0,  0,  0,\n",
       "           5,  2,  3,  3,  0,  0,  2,  2,  2,  1,  0,  0,  3,  2,  3,  2,  6,  2,\n",
       "           1,  3], device='cuda:0')}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Union, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BBBNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hydraulics2id: Dict[str, int],\n",
    "        enclosure2id: Dict[str, int],\n",
    "        productgroup2id: Dict[str, int],\n",
    "        hydraulics_proj_dim: Optional[int] = 2,\n",
    "        enclosure_proj_dim: Optional[int] = 2,\n",
    "        productgroup_proj_dim: Optional[int] = 2,\n",
    "        dense_layers: List[int] = [32],  # List of layer sizes\n",
    "        dropout_rates: Union[float, List[float]] = 0.1,  # Single or per-layer dropout rates\n",
    "        use_batch_norm: bool = True,\n",
    "        activation: str = \"gelu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embeddings_hydraulics = nn.Embedding(num_embeddings=len(hydraulics2id), embedding_dim=hydraulics_proj_dim)\n",
    "        self.embeddings_enclosure = nn.Embedding(num_embeddings=len(enclosure2id), embedding_dim=enclosure_proj_dim)\n",
    "        self.embeddings_productgroup = nn.Embedding(num_embeddings=len(productgroup2id), embedding_dim=productgroup_proj_dim)\n",
    "        \n",
    "        # Calculate total input features\n",
    "        self.total_features = 11 + hydraulics_proj_dim + enclosure_proj_dim + productgroup_proj_dim\n",
    "        \n",
    "        # Process dense layer configuration\n",
    "        self.dense_layers = dense_layers\n",
    "        if not dense_layers:\n",
    "            raise ValueError(\"dense_layers must contain at least one layer size\")\n",
    "            \n",
    "        # Process dropout rates\n",
    "        if isinstance(dropout_rates, float):\n",
    "            self.dropout_rates = [dropout_rates] * len(dense_layers)\n",
    "        else:\n",
    "            if len(dropout_rates) != len(dense_layers):\n",
    "                raise ValueError(\"dropout_rates must match length of dense_layers\")\n",
    "            self.dropout_rates = dropout_rates\n",
    "            \n",
    "        # Set activation function\n",
    "        self.activation = self._get_activation(activation)\n",
    "        \n",
    "        # Create dense blocks\n",
    "        self.dense_blocks = nn.ModuleList()\n",
    "        in_features = self.total_features\n",
    "        \n",
    "        for i, out_features in enumerate(dense_layers):\n",
    "            block = nn.ModuleDict()\n",
    "            \n",
    "            # Linear layer\n",
    "            block['linear'] = nn.Linear(in_features, out_features)\n",
    "            \n",
    "            # BatchNorm (optional)\n",
    "            if use_batch_norm:\n",
    "                block['norm'] = nn.BatchNorm1d(out_features)\n",
    "            \n",
    "            # Dropout\n",
    "            block['dropout'] = nn.Dropout(self.dropout_rates[i])\n",
    "            \n",
    "            self.dense_blocks.append(block)\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Linear(in_features, 1)\n",
    "        \n",
    "    def _get_activation(self, activation: str) -> nn.Module:\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'gelu': nn.GELU(),\n",
    "            'leaky_relu': nn.LeakyReLU(),\n",
    "            'selu': nn.SELU(),\n",
    "            'silu': nn.SiLU()\n",
    "        }\n",
    "        return activations.get(activation.lower(), nn.GELU())\n",
    "    \n",
    "    def forward(self, inputs: Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]): \n",
    "        # Process embeddings\n",
    "        emb_h = self.embeddings_hydraulics(inputs['X_cat']['Hydraulics'])\n",
    "        emb_e = self.embeddings_enclosure(inputs['X_cat']['Enclosure'])\n",
    "        emb_pg = self.embeddings_productgroup(inputs['X_cat']['ProductGroup'])\n",
    "        \n",
    "        # Concatenate all features\n",
    "        x = torch.cat([inputs['X_float'], emb_h, emb_e, emb_pg], dim=1)\n",
    "        \n",
    "        # Process through dense blocks\n",
    "        for block in self.dense_blocks:\n",
    "            x = block['linear'](x)\n",
    "            \n",
    "            if 'norm' in block:\n",
    "                x = block['norm'](x)\n",
    "            \n",
    "            x = block['dropout'](x)    \n",
    "            x = self.activation(x)\n",
    "            \n",
    "        \n",
    "        # Final output\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Check implementation\n",
    "model = BBBNet(\n",
    "    hydraulics2id=hydraulics2id,\n",
    "    enclosure2id=enclosure2id,\n",
    "    productgroup2id=productgroup2id,\n",
    "    dense_layers=[64],  # Three hidden layers\n",
    "    dropout_rates=0.1,  # Different dropout per layer\n",
    "    use_batch_norm=True,\n",
    "    activation='gelu',\n",
    ")\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_logits = model(batch_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts\n",
    "\n",
    "# Loss & optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3, weight_decay=1e-2)\n",
    "\n",
    "# Scheduler\n",
    "scheduler_type = \"step\"\n",
    "steps_per_epoch = len(train_dtl)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=steps_per_epoch, T_mult=2, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== METRICS ======\n",
    "def mean_absolute_percentage_error(preds, targets, epsilon=1e-8):\n",
    "    \"\"\"Calculates MAPE (%)\"\"\"\n",
    "    return torch.mean(torch.abs((targets - preds) / (targets + epsilon))) * 100\n",
    "\n",
    "# ====== TRAINING LOOP ======\n",
    "def train_model_wandb(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    eval_loader,\n",
    "    loss_module,\n",
    "    num_epochs=100,\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "):\n",
    "    wandb.init(project=\"bbb-regression\", config={\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"architecture\": \"DNN-Regressor\",\n",
    "        \"loss\": \"MSE\",\n",
    "        \"scheduler\": \"CosineAnnealingWarmRestarts\",\n",
    "    })\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    best_eval_loss = float('inf')  # Tracks best validation loss\n",
    "    best_eval_mape = float('inf')  # Tracks best validation MAPE\n",
    "    patience_counter = 0  # Counts epochs without improvement\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "        # ===== TRAINING PHASE =====\n",
    "        model.train()\n",
    "        train_loss, train_mape = 0.0, 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            inputs = {\n",
    "                \"X_float\": data[\"X_float\"].to(device),\n",
    "                \"X_cat\": {k: v.to(device) for k, v in data[\"X_cat\"].items()}\n",
    "            }\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(inputs).squeeze(-1)  # Ensure shape [batch_size]\n",
    "            \n",
    "            # Compute loss & metrics\n",
    "            loss = loss_module(preds, targets)\n",
    "            batch_mape = mean_absolute_percentage_error(preds, targets)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Step per batch for CosineAnnealingWarmRestarts\n",
    "\n",
    "            # Accumulate metrics\n",
    "            train_loss += loss.item()\n",
    "            train_mape += batch_mape.item()\n",
    "\n",
    "            # Log batch metrics\n",
    "            wandb.log({\n",
    "                \"batch_train_loss\": loss.item(),\n",
    "                \"batch_train_mape\": batch_mape.item(),\n",
    "                \"lr\": optimizer.param_groups[0]['lr'],\n",
    "            })\n",
    "\n",
    "        # Epoch metrics\n",
    "        epoch_train_loss = train_loss / len(train_dtl)\n",
    "        epoch_train_mape = train_mape / len(train_dtl)\n",
    "\n",
    "        # ===== EVALUATION PHASE =====\n",
    "        model.eval()\n",
    "        eval_loss, eval_mape = 0.0, 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, targets in eval_loader:\n",
    "                inputs = {\n",
    "                    \"X_float\": data[\"X_float\"].to(device),\n",
    "                    \"X_cat\": {k: v.to(device) for k, v in data[\"X_cat\"].items()}\n",
    "                }\n",
    "                targets = targets.to(device)\n",
    "                preds = model(inputs).squeeze(-1)\n",
    "\n",
    "                eval_loss += loss_module(preds, targets).item()\n",
    "                eval_mape += mean_absolute_percentage_error(preds, targets).item()\n",
    "\n",
    "        epoch_eval_loss = eval_loss / len(val_dtl)\n",
    "        epoch_eval_mape = eval_mape / len(val_dtl)\n",
    "\n",
    "        # Log epoch metrics\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": epoch_train_loss,\n",
    "            \"train_mape\": epoch_train_mape,\n",
    "            \"eval_loss\": epoch_eval_loss,\n",
    "            \"eval_mape\": epoch_eval_mape,\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {epoch_train_loss:.4f} (MAPE: {epoch_train_mape:.2f}%) | \"\n",
    "            f\"Val Loss: {epoch_eval_loss:.4f} (MAPE: {epoch_eval_mape:.2f}%)\"\n",
    "        )\n",
    "\n",
    "        # ===== Early Stopping Check =====\n",
    "        mape_improved = (epoch_eval_mape < best_eval_mape - min_delta)\n",
    "        loss_improved = (epoch_eval_loss < best_eval_loss - min_delta)\n",
    "\n",
    "        if mape_improved or loss_improved:\n",
    "            # Update whichever metric improved\n",
    "            if mape_improved:\n",
    "                best_eval_mape = epoch_eval_mape\n",
    "            if loss_improved:\n",
    "                best_eval_loss = epoch_eval_loss\n",
    "\n",
    "            patience_counter = 0  # Reset counter\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"New best: Loss={best_eval_loss:.4f}, MAPE={best_eval_mape:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping! No improvement for {patience} epochs.\")\n",
    "                print(f\"Final metrics - Loss: {epoch_eval_loss:.4f}, MAPE: {epoch_eval_mape:.2f}%\")\n",
    "                break\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_wandb(model, optimizer, train_dtl, val_dtl, loss_fn, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
